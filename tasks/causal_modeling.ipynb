{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d349b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a notebook to perform causal ML analysis with WNV. Since classical modeling techniques were\n",
    "not sufficient at predicting WNV, I want to try using causal ML to help highlight some features that\n",
    "are properly tested to *cause* WNV.\n",
    "\n",
    "NOTE: Performing classical modeling did help to highlight certain features that were important in\n",
    "the prediction, but these may be based heavily on correlation, not causation. Technically, I could\n",
    "also go backwards and extract the observations where the SHAP values were most significant, but \n",
    "this is a bit of a hassle, and I find causal ML to be more imformative.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8d4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from task 2\n",
    "train_data = pd.read_pickle(\"../data/train.pkl\")\n",
    "valid_data = pd.read_pickle(\"../data/valid.pkl\")\n",
    "test_data = pd.read_pickle(\"../data/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ac47d",
   "metadata": {},
   "source": [
    "For this, I will apply a Double Machine Learning (DML) model. This works by combining two ML models,\n",
    "one to explain the target (in this case, when WNV is present), and another to explain some sort of\n",
    "treatment (which would be one of the features, such as if rain was present). You compare the \n",
    "unexplained part of the target with the unexplained part of the chosen treatment, providing an \n",
    "unbiased estimate of the effect on the treatment on WNV being present.\n",
    "\n",
    "An analogy to help make this sense is like trying to clean off fog on a lens:\n",
    "- One ML model is used to clean the fog off `Y` (removing what `X`, the cofounders, explains about \n",
    "  `Y`)\n",
    "- Another ML model is used to clean the fog off `T` (removing what `X` explains about `T`)\n",
    "- Then, you just compare the clean part of `T` and `Y` to measure the true effect\n",
    "\n",
    "Overall this is a nice way to remove confouding variables (variables that impact both the treatment\n",
    "and the target)!\n",
    "\n",
    "-----------\n",
    "\n",
    "More formally, the two models are combined by calculating the residuals of both initial models, then\n",
    "fitting a regression using both residuals.\n",
    "\n",
    "$$\\tilde{Y} = \\theta \\cdot \\tilde{T} + \\text{noise}$$\n",
    "\n",
    "where $$\\theta$$ is the estimated causal effect of the treatment on the outcome.\n",
    "\n",
    "This final regression step is **where the two models come together** â€” by comparing what's left \n",
    "(the residuals), DML isolates the causal effect without confounding bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81421512",
   "metadata": {},
   "source": [
    "However, there are four main assumptions for applying this model:\n",
    "1. Unconfoundness - All cofounders are accounting for when fitting a DML model. I'll do my best to \n",
    "   follow this assumption by including all possible features when fitting each individual DML model.\n",
    "2. Overlap - No group is perfectly treated or untreated, so we can compare treated/untreated \n",
    "   observations with similar covariates. I'll check for this via AUC.\n",
    "3. Consistency - There is no ambiguity in the treatment labels, which I'll ensure based on how I\n",
    "   implement the models, but this is an easy assumption to uphold generally.\n",
    "4. Stable Unit Treatment Value Assumption - The outcome is only affected by the treatment assigned\n",
    "   to that observation, and not by treatments on other observations. I know this is true based on \n",
    "   data preprocessing from task 2.\n",
    "\n",
    "Assumption 2 must be checked for however. This is done by estimating propensity scores, the \n",
    "probability of receiving treatment given the covariates `X`. I'll do this with logistic regression\n",
    "with AUC scores, and show the columns that fit the threshold below:\n",
    "\n",
    "- If AUC is roughly equal to 0.5, the treatment is not very predictable on X, meaning there is \n",
    "   overlap.\n",
    "- AUC values greater than ~0.7 or less than ~0.3 imply that the model can (almost) perfectly predict\n",
    "  treatment, suggesting poor overlap, meaning the assumption is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee624cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train_data, valid_data, and test_data into one DataFrame\n",
    "# For causal ML, since there is no \"prediction\", there is no need to have our standard data split\n",
    "df = pd.concat([train_data, valid_data, test_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Treatment     AUC\n",
      "0    ResultDir  0.4279\n",
      "1      Sunrise  0.4584\n",
      "2           RA  0.4733\n",
      "3         Cool  0.4912\n",
      "4           BR  0.5570\n",
      "5           HZ  0.4399\n",
      "6           TS  0.5028\n",
      "7         Heat  0.5185\n",
      "8  day_of_year  0.4093\n"
     ]
    }
   ],
   "source": [
    "# Define outcome and features\n",
    "outcome_col = 'WnvPresent'\n",
    "excluded_cols = [outcome_col]\n",
    "feature_cols = [col for col in df.columns if col not in excluded_cols]\n",
    "\n",
    "# Separate out candidate treatments (binary)\n",
    "candidate_treatments = [col for col in df.columns if col not in excluded_cols and\n",
    "                        df[col].nunique() == 2 and set(df[col].unique()) <= {0, 1}]\n",
    "\n",
    "# Storage for valid treatments\n",
    "valid_treatments = []\n",
    "\n",
    "# Loop over treatments\n",
    "for treatment_col in candidate_treatments:\n",
    "    X = df.drop(columns=[outcome_col, treatment_col])\n",
    "    T = df[treatment_col].values\n",
    "\n",
    "    # Scale features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Fit logistic regression\n",
    "    try:\n",
    "        ps_model = LogisticRegression(max_iter=2000)\n",
    "        ps_model.fit(X_scaled, T)\n",
    "        propensity_scores = ps_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = roc_auc_score(T, propensity_scores)\n",
    "\n",
    "        # Apply overlap threshold rule: AUC should be not too close to 1 or 0\n",
    "        if 0.3 < auc < 0.7:\n",
    "            valid_treatments.append((treatment_col, auc))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model for {treatment_col}: {e}\")\n",
    "\n",
    "# Create DataFrame of valid treatments\n",
    "valid_treatment_df = pd.DataFrame(valid_treatments, columns=[\"Treatment\", \"AUC\"])\n",
    "valid_treatment_df.sort_values(\"AUC\", inplace=True)\n",
    "valid_treatment_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f803a",
   "metadata": {},
   "source": [
    "These are the remaining features/variables that we can properly test individually as treatment. I'll\n",
    "filter `df` to just these columns with `WnvPresent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       WnvPresent\n",
      "0               0\n",
      "1               0\n",
      "2               0\n",
      "3               0\n",
      "4               0\n",
      "...           ...\n",
      "10408           0\n",
      "10409           0\n",
      "10410           0\n",
      "10411           0\n",
      "10412           0\n",
      "\n",
      "[10413 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df[list(valid_treatment_df[\"Treatment\"]) + [\"WnvPresent\"]]\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
