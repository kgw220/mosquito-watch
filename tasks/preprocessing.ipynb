{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ba3a10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file performs preprocessing on the relevant data, to prepare for model building.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing, tree, ensemble, linear_model, metrics, model_selection, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "46ed84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twentyfour_hour_to_float(HHMM: str) -> float:\n",
    "    \"\"\"\n",
    "    Converts a time in HHMM format to a float representing hours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    HHMM : str\n",
    "        A string representing time in HHMM format (e.g., '1230' for 12:30 PM).\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        A float representing the time in hours. If HHMM is '-', returns NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    if HHMM == '-':\n",
    "        return np.nan\n",
    "    else:\n",
    "        hour = float(HHMM[:2])\n",
    "        minute = float(HHMM[2:])\n",
    "        return hour + minute/60\n",
    "    \n",
    "\n",
    "def avg_wind_direction(S1: Union[float, np.nan], S2: Union[float, np.nan]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average wind direction from two stations, accounting for circular nature of wind direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : float or np.nan\n",
    "        Wind direction from Station 1 (in 10-degree increments, 0-35 scale).\n",
    "    S2 : float or np.nan\n",
    "        Wind direction from Station 2 (in 10-degree increments, 0-35 scale).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Averaged wind direction. Handles missing values and circular differences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle missing values\n",
    "    if S1 == np.nan:\n",
    "        avg = float(S2)\n",
    "    elif S2 == np.nan:\n",
    "        avg = float(S1)\n",
    "        \n",
    "    # Handle circular difference (i.e., crossing the 0/36 boundary)\n",
    "    elif np.abs(S1 - S2) > 18:\n",
    "        if S1 < S2:\n",
    "            S1 += 36\n",
    "        else:\n",
    "            S2 += 36\n",
    "        avg = (S1 + S2) / 2.0\n",
    "    else:\n",
    "        avg = (S1 + S2) / 2.0\n",
    "\n",
    "    # Adjust back if average exceeds 36 (due to circular adjustment)\n",
    "    if avg >= 36:\n",
    "        avg -= 36\n",
    "        \n",
    "    return avg\n",
    "\n",
    "\n",
    "def combine_conditions(S1: str, S2: str) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Combines weather condition codes from two stations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : str\n",
    "        Condition string from Station 1 (space-separated codes).\n",
    "    S2 : str\n",
    "        Condition string from Station 2 (space-separated codes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or list\n",
    "        Combined unique condition codes. Returns empty string if both are empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle empty conditions from both stations\n",
    "    if S1 == \" \" and S2 == \" \":\n",
    "        return \"\"\n",
    "    elif S1 == \" \":\n",
    "        return S2\n",
    "    elif S2 == \" \":\n",
    "        return S1\n",
    "    else:\n",
    "        # Split condition codes into lists and combine unique codes\n",
    "        S1_list = S1.split(\" \")\n",
    "        S2_list = S2.split(\" \")\n",
    "        return list(set(S1_list + S2_list))\n",
    "\n",
    "\n",
    "def avg_col(S1: Union[float, np.nan], S2: Union[float, np.nan]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the average of two numeric values, handling missing data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : float or np.nan\n",
    "        Value from Station 1.\n",
    "    S2 : float or np.nan\n",
    "        Value from Station 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Average of S1 and S2, or the non-missing value if one is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle missing values\n",
    "    if S1 == np.nan:\n",
    "        avg = float(S2)\n",
    "    elif S2 == np.nan:\n",
    "        avg = float(S1)\n",
    "    else:\n",
    "        avg = (S1 + S2) / 2.0\n",
    "        \n",
    "    return avg\n",
    "\n",
    "\n",
    "def find_zip(address: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the ZIP code from an address string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    address : str\n",
    "        The address string containing the ZIP code.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The extracted ZIP code if found, otherwise an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if re.search('(?<=IL )[0-9]*', address):\n",
    "        return re.search('(?<=IL )[0-9]*', address).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e9442aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_pickle(\"../data/weather.pkl\")\n",
    "mosquito = pd.read_pickle(\"../data/mosquito.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "36032da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in each file to datetime objects\n",
    "for df in [weather, mosquito]:\n",
    "    df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3040108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a similar way, many other variables in the weather dataframe are objects due to the values that\n",
    "# some observations have, such as \"M\" for missing values. These will be converted to numeric.\n",
    "num_vars = ['Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat', 'Cool', 'PrecipTotal',\n",
    "                 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', 'AvgSpeed']\n",
    "for var in num_vars:\n",
    "    weather[var] = pd.to_numeric(weather[var], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "dfdaa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time features for time columns\n",
    "weather.Sunrise = weather.Sunrise.apply(twentyfour_hour_to_float)\n",
    "weather.Sunset = weather.Sunset.apply(twentyfour_hour_to_float)\n",
    "\n",
    "# Calculate hours in day\n",
    "weather[\"hours_in_day\"] = weather.Sunset - weather.Sunrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "22188b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns since they are mostly all NaNs\n",
    "weather.drop(['Depth', 'Water1', 'SnowFall'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4e066442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, the data from both weather stations will be combined\n",
    "\n",
    "# Initialize dataframe to hold combined weather data\n",
    "combined_weather = pd.DataFrame()\n",
    "\n",
    "# Split the weather data into two stations\n",
    "s1 = weather[weather.Station==1]\n",
    "s2 = weather[weather.Station==2]\n",
    "\n",
    "# Set columns that only have data for Station 1 or the data being the same.\n",
    "for col in ['Date', 'Depart', 'Sunrise', 'Sunset', 'hours_in_day']:\n",
    "    combined_weather[col] = s1[col].values\n",
    "\n",
    "# Apply average functions to the columns that have data for both stations\n",
    "for col in ['Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'Heat', 'Cool', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'AvgSpeed']:\n",
    "    combined_weather[col] = [avg_col(a, b) for a, b in zip(s1[col].values, s2[col].values)]\n",
    "\n",
    "# Applying related weather average functions\n",
    "combined_weather['ResultDir'] = [avg_wind_direction(a, b) for a, b in zip(s1.ResultDir.values, s2.ResultDir.values)]\n",
    "combined_weather['CodeSum'] = [combine_conditions(a, b) for a, b in zip(s1.CodeSum.values, s2.CodeSum.values)]\n",
    "\n",
    "# PrecipTotal for each station is too different, so I keep both separate.\n",
    "combined_weather['PrecipTotal_station1'] = weather.PrecipTotal[weather.Station==1].values\n",
    "combined_weather['PrecipTotal_station2'] = weather.PrecipTotal[weather.Station==2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bc84d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the precipitation totals have too many missing values to be worth imputing in an accurate \n",
    "# sense. They will be dropped here.\n",
    "combined_weather.drop(['PrecipTotal_station1', 'PrecipTotal_station2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9aefa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I impute missing values for some columns. I try using a unique strategy, where I train a \n",
    "# small model to predict the missing values based on the other columns. This is done for the\n",
    "# WetBulb and StnPressure columns, which have a significant number of missing values.\n",
    "\n",
    "# TODO: Perhaps perform this step after merging with mosquito data to avoid data leakage\n",
    "\n",
    "# WetBulb\n",
    "X = combined_weather.dropna().drop(['Date', 'WetBulb', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['WetBulb']\n",
    "\n",
    "dt_WetBulb = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "dt_WetBulb.fit(X, y)\n",
    "\n",
    "# Predict missing WetBulb values\n",
    "mask_wb_null = combined_weather['WetBulb'].isnull()\n",
    "X_pred = combined_weather.loc[mask_wb_null, X.columns]\n",
    "predicted_WetBulb = dt_WetBulb.predict(X_pred)\n",
    "combined_weather.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\n",
    "\n",
    "# StnPressure\n",
    "X = combined_weather.dropna().drop(['Date', 'StnPressure', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['StnPressure']\n",
    "\n",
    "xb_StnPressure = xgboost.XGBRegressor()\n",
    "xb_StnPressure.fit(X, y)\n",
    "\n",
    "# Predict missing StnPressure values\n",
    "mask_sp_null = combined_weather['StnPressure'].isnull()\n",
    "X_pred = combined_weather.loc[mask_sp_null, X.columns]\n",
    "predicted_StnPressure = xb_StnPressure.predict(X_pred)\n",
    "combined_weather.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7906d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of year, month, day of week, and year from date column\n",
    "mosquito['day_of_year'] = mosquito.Date.dt.dayofyear\n",
    "mosquito['month'] = mosquito.Date.dt.month\n",
    "mosquito['day_of week'] = mosquito.Date.dt.dayofweek\n",
    "mosquito['year'] = mosquito.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1e3bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the mosquito data is time based, it would be bad to split randomly, since this can cause \n",
    "# data leakage. Instead, I will split the data into train, validation, and test sets based on time.\n",
    "# Train data covers 2007/2009, validation data covers 2011, and test data covers 2013.\n",
    "train = mosquito[mosquito['Date'].dt.year.isin([2007, 2009])]\n",
    "valid = mosquito[mosquito['Date'].dt.year == 2011]\n",
    "test = mosquito[mosquito['Date'].dt.year == 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f7d41487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['zip_code'] = train.Address.apply(find_zip)\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid['zip_code'] = valid.Address.apply(find_zip)\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['zip_code'] = test.Address.apply(find_zip)\n"
     ]
    }
   ],
   "source": [
    "# Extract zip codes; this will be the main location based feature outside of lat/long\n",
    "train['zip_code'] = train.Address.apply(find_zip)\n",
    "valid['zip_code'] = valid.Address.apply(find_zip)\n",
    "test['zip_code'] = test.Address.apply(find_zip)\n",
    "\n",
    "# Address, Street, and AddressNumberAndStreet provide the same information, so they are removed to \n",
    "# avoid redundancy.\n",
    "train = train.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)\n",
    "valid = valid.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)\n",
    "test = test.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "20f75c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing zip codes based on lat/long. A unique strategy is used here, where I train a \n",
    "# model to do this based on the training latitude and longitude data.\n",
    "X = train.dropna(subset=['Latitude', 'Longitude', 'zip_code'])[['Latitude', 'Longitude']]\n",
    "y = train.dropna(subset=['Latitude', 'Longitude', 'zip_code'])['zip_code']\n",
    "\n",
    "# Fit model to predict zip codes based on lat/long\n",
    "dtree_forzip = tree.DecisionTreeClassifier()\n",
    "dtree_forzip.fit(X, y)\n",
    "\n",
    "# Define a mask to look for rows with missing zip codes, and predict them for train set\n",
    "mask_zip_null = train['zip_code'].isnull()\n",
    "X_missing_zip = train.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "train.loc[mask_zip_null, 'zip_code'] = predicted_zip\n",
    "\n",
    "# Repeat for valid and test sets\n",
    "mask_zip_null = valid['zip_code'].isnull()\n",
    "X_missing_zip = valid.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "valid.loc[mask_zip_null, 'zip_code'] = predicted_zip\n",
    "\n",
    "mask_zip_null = test['zip_code'].isnull()\n",
    "X_missing_zip = test.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "test.loc[mask_zip_null, 'zip_code'] = predicted_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e7efc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I create some new features based on trap data. The idea is to create features that capture \n",
    "# the distribution of mosquitos and WNV presence across different traps. This can help the model \n",
    "# understand the relative importance of different traps in terms of mosquito counts and WNV \n",
    "# presence.\n",
    "num_by_trap = train[['Trap', 'NumMosquitos', 'WnvPresent']].groupby('Trap').agg('sum')\n",
    "num_by_trap['trap_percent_of_all_mosquitos'] = num_by_trap['NumMosquitos']/sum(num_by_trap.NumMosquitos)\n",
    "num_by_trap['trap_percent_with_wnv'] = num_by_trap.WnvPresent/num_by_trap.NumMosquitos\n",
    "num_by_trap.reset_index(inplace=True)\n",
    "\n",
    "# Define dictionaries to map trap names to their respective weights\n",
    "map_mosq_weight = {t:v for t, v in zip(num_by_trap.Trap.values, num_by_trap['trap_percent_of_all_mosquitos'].values)}\n",
    "map_wnv_weight = {t:v for t, v in zip(num_by_trap.Trap.values, num_by_trap['trap_percent_with_wnv'].values)}\n",
    "\n",
    "# Create a second mapping for WNV presence, which is the average per-record WNV presence per trap\n",
    "map_wnv_weight2 = {}\n",
    "for trap in set(train.Trap):\n",
    "    map_wnv_weight2[trap] = sum(train.WnvPresent[train.Trap==trap]/sum(train.Trap==trap))\n",
    "map_wnv_weight2\n",
    "\n",
    "# Map the trap weights to the train, valid, and test sets\n",
    "train['trap_percent_of_all_mosquitos'] = train.Trap.map(map_mosq_weight)\n",
    "train['trap_percent_with_wnv'] = train.Trap.map(map_wnv_weight)\n",
    "train['trap_percent_with_wnv2'] = train.Trap.map(map_wnv_weight2)\n",
    "\n",
    "valid['trap_percent_of_all_mosquitos'] = valid.Trap.map(map_mosq_weight).fillna(0)\n",
    "valid['trap_percent_with_wnv'] = valid.Trap.map(map_wnv_weight).fillna(0)\n",
    "valid['trap_percent_with_wnv2'] = valid.Trap.map(map_wnv_weight2).fillna(0)\n",
    "\n",
    "test['trap_percent_of_all_mosquitos'] = test.Trap.map(map_mosq_weight).fillna(0)\n",
    "test['trap_percent_with_wnv'] = test.Trap.map(map_wnv_weight).fillna(0)\n",
    "test['trap_percent_with_wnv2'] = test.Trap.map(map_wnv_weight2).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "84fecd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the mosquito data with the combined weather data, left joining on the date column\n",
    "train_w = pd.merge(train, combined_weather, how='left', on='Date')\n",
    "valid_w = pd.merge(valid, combined_weather, how='left', on='Date')\n",
    "test_w = pd.merge(test, combined_weather, how='left', on='Date')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
