{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba3a10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file performs preprocessing on the relevant data, to prepare for model building.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing, tree, ensemble, linear_model, metrics, model_selection, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "46ed84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twentyfour_hour_to_float(HHMM: str) -> float:\n",
    "    \"\"\"\n",
    "    Converts a time in HHMM format to a float representing hours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    HHMM : str\n",
    "        A string representing time in HHMM format (e.g., '1230' for 12:30 PM).\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        A float representing the time in hours. If HHMM is '-', returns NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    if HHMM == '-':\n",
    "        return np.nan\n",
    "    else:\n",
    "        hour = float(HHMM[:2])\n",
    "        minute = float(HHMM[2:])\n",
    "        return hour + minute/60\n",
    "    \n",
    "\n",
    "def avg_wind_direction(S1: Union[float, np.nan], S2: Union[float, np.nan]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average wind direction from two stations, accounting for circular nature of wind direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : float or np.nan\n",
    "        Wind direction from Station 1 (in 10-degree increments, 0-35 scale).\n",
    "    S2 : float or np.nan\n",
    "        Wind direction from Station 2 (in 10-degree increments, 0-35 scale).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Averaged wind direction. Handles missing values and circular differences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle missing values\n",
    "    if S1 == np.nan:\n",
    "        avg = float(S2)\n",
    "    elif S2 == np.nan:\n",
    "        avg = float(S1)\n",
    "        \n",
    "    # Handle circular difference (i.e., crossing the 0/36 boundary)\n",
    "    elif np.abs(S1 - S2) > 18:\n",
    "        if S1 < S2:\n",
    "            S1 += 36\n",
    "        else:\n",
    "            S2 += 36\n",
    "        avg = (S1 + S2) / 2.0\n",
    "    else:\n",
    "        avg = (S1 + S2) / 2.0\n",
    "\n",
    "    # Adjust back if average exceeds 36 (due to circular adjustment)\n",
    "    if avg >= 36:\n",
    "        avg -= 36\n",
    "        \n",
    "    return avg\n",
    "\n",
    "\n",
    "def combine_conditions(S1: str, S2: str) -> Union[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Combines weather condition codes from two stations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : str\n",
    "        Condition string from Station 1 (space-separated codes).\n",
    "    S2 : str\n",
    "        Condition string from Station 2 (space-separated codes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or list\n",
    "        Combined unique condition codes. Returns empty string if both are empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle empty conditions from both stations\n",
    "    if S1 == \" \" and S2 == \" \":\n",
    "        return \"\"\n",
    "    elif S1 == \" \":\n",
    "        return S2\n",
    "    elif S2 == \" \":\n",
    "        return S1\n",
    "    else:\n",
    "        # Split condition codes into lists and combine unique codes\n",
    "        S1_list = S1.split(\" \")\n",
    "        S2_list = S2.split(\" \")\n",
    "        return list(set(S1_list + S2_list))\n",
    "\n",
    "\n",
    "def avg_col(S1: Union[float, np.nan], S2: Union[float, np.nan]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the average of two numeric values, handling missing data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    S1 : float or np.nan\n",
    "        Value from Station 1.\n",
    "    S2 : float or np.nan\n",
    "        Value from Station 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Average of S1 and S2, or the non-missing value if one is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle missing values\n",
    "    if S1 == np.nan:\n",
    "        avg = float(S2)\n",
    "    elif S2 == np.nan:\n",
    "        avg = float(S1)\n",
    "    else:\n",
    "        avg = (S1 + S2) / 2.0\n",
    "        \n",
    "    return avg\n",
    "\n",
    "\n",
    "def find_zip(address: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the ZIP code from an address string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    address : str\n",
    "        The address string containing the ZIP code.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The extracted ZIP code if found, otherwise an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if re.search('(?<=IL )[0-9]*', address):\n",
    "        return re.search('(?<=IL )[0-9]*', address).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9442aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Incorporate spray data\n",
    "spray = pd.read_pickle(\"../data/spray.pkl\")\n",
    "weather = pd.read_pickle(\"../data/weather.pkl\")\n",
    "mosquito = pd.read_pickle(\"../data/mosquito_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "36032da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in each file to datetime objects\n",
    "for df in [spray, weather, mosquito]:\n",
    "    df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3040108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a similar way, many other variables in the weather dataframe are objects due to the values that\n",
    "# some observations have, such as \"M\" for missing values. These will be converted to numeric.\n",
    "num_vars = ['Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat', 'Cool', 'PrecipTotal',\n",
    "                 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir', 'AvgSpeed']\n",
    "for var in num_vars:\n",
    "    weather[var] = pd.to_numeric(weather[var], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dfdaa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time features for time columns\n",
    "weather.Sunrise = weather.Sunrise.apply(twentyfour_hour_to_float)\n",
    "weather.Sunset = weather.Sunset.apply(twentyfour_hour_to_float)\n",
    "\n",
    "# Calculate hours in day\n",
    "weather[\"hours_in_day\"] = weather.Sunset - weather.Sunrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "22188b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns since they are mostly all NaNs\n",
    "weather.drop(['Depth', 'Water1', 'SnowFall'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4e066442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, the data from both weather stations will be combined\n",
    "\n",
    "# Initialize dataframe to hold combined weather data\n",
    "combined_weather = pd.DataFrame()\n",
    "\n",
    "# Split the weather data into two stations\n",
    "s1 = weather[weather.Station==1]\n",
    "s2 = weather[weather.Station==2]\n",
    "\n",
    "# Set columns that only have data for Station 1 or the data being the same.\n",
    "for col in ['Date', 'Depart', 'Sunrise', 'Sunset', 'hours_in_day']:\n",
    "    combined_weather[col] = s1[col].values\n",
    "\n",
    "# Apply average functions to the columns that have data for both stations\n",
    "for col in ['Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'Heat', 'Cool', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'AvgSpeed']:\n",
    "    combined_weather[col] = [avg_col(a, b) for a, b in zip(s1[col].values, s2[col].values)]\n",
    "\n",
    "# Applying related weather average functions\n",
    "combined_weather['ResultDir'] = [avg_wind_direction(a, b) for a, b in zip(s1.ResultDir.values, s2.ResultDir.values)]\n",
    "combined_weather['CodeSum'] = [combine_conditions(a, b) for a, b in zip(s1.CodeSum.values, s2.CodeSum.values)]\n",
    "\n",
    "# PrecipTotal for each station is too different, so I keep both separate.\n",
    "combined_weather['PrecipTotal_station1'] = weather.PrecipTotal[weather.Station==1].values\n",
    "combined_weather['PrecipTotal_station2'] = weather.PrecipTotal[weather.Station==2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bc84d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the precipitation totals have too many missing values to be worth imputing in an accurate \n",
    "# sense. They will be dropped here.\n",
    "combined_weather.drop(['PrecipTotal_station1', 'PrecipTotal_station2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9aefa344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# WetBulb\\nX = combined_weather.dropna().drop(['Date', 'WetBulb', 'CodeSum'], axis=1)\\ny = combined_weather.dropna()['WetBulb']\\n\\ndt_WetBulb = ensemble.RandomForestRegressor(n_estimators=100)\\ndt_WetBulb.fit(X, y)\\n\\n# Predict missing WetBulb values\\nmask_wb_null = combined_weather['WetBulb'].isnull()\\nX_pred = combined_weather.loc[mask_wb_null, X.columns]\\npredicted_WetBulb = dt_WetBulb.predict(X_pred)\\n\\ncombined_weather.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\\n\\n# StnPressure\\nX = combined_weather.dropna().drop(['Date', 'StnPressure', 'CodeSum'], axis=1)\\ny = combined_weather.dropna()['StnPressure']\\n\\nxb_StnPressure = xgboost.XGBRegressor()\\nxb_StnPressure.fit(X, y)\\n\\n# Predict missing StnPressure values\\nmask_sp_null = combined_weather['StnPressure'].isnull()\\nX_pred = combined_weather.loc[mask_sp_null, X.columns]\\npredicted_StnPressure = xb_StnPressure.predict(X_pred)\\n\\ncombined_weather.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure\\n\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, I impute missing values for some columns. I try using a unique strategy, where I train a \n",
    "# small model to predict the missing values based on the other columns. This is done for the\n",
    "# WetBulb and StnPressure columns, which have a significant number of missing values.\n",
    "\"\"\"\n",
    "# WetBulb\n",
    "X = combined_weather.dropna().drop(['Date', 'WetBulb', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['WetBulb']\n",
    "\n",
    "dt_WetBulb = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "dt_WetBulb.fit(X, y)\n",
    "\n",
    "# Predict missing WetBulb values\n",
    "mask_wb_null = combined_weather['WetBulb'].isnull()\n",
    "X_pred = combined_weather.loc[mask_wb_null, X.columns]\n",
    "predicted_WetBulb = dt_WetBulb.predict(X_pred)\n",
    "\n",
    "combined_weather.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\n",
    "\n",
    "# StnPressure\n",
    "X = combined_weather.dropna().drop(['Date', 'StnPressure', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['StnPressure']\n",
    "\n",
    "xb_StnPressure = xgboost.XGBRegressor()\n",
    "xb_StnPressure.fit(X, y)\n",
    "\n",
    "# Predict missing StnPressure values\n",
    "mask_sp_null = combined_weather['StnPressure'].isnull()\n",
    "X_pred = combined_weather.loc[mask_sp_null, X.columns]\n",
    "predicted_StnPressure = xb_StnPressure.predict(X_pred)\n",
    "\n",
    "combined_weather.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fd19d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2007-05-29\n",
       "1        2007-05-29\n",
       "2        2007-05-29\n",
       "3        2007-05-29\n",
       "4        2007-05-29\n",
       "            ...    \n",
       "116288   2014-10-02\n",
       "116289   2014-10-02\n",
       "116290   2014-10-02\n",
       "116291   2014-10-02\n",
       "116292   2014-10-02\n",
       "Name: Date, Length: 126799, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mosquito.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2cb39",
   "metadata": {},
   "source": [
    "Train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e25e4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the mosquito data is time based, it would be bad to split randomly, since this can cause \n",
    "# data leakage. Instead, I will split the data into train, validation, and test sets based on time.\n",
    "# Train data covers 2007-2011, validation data covers 2012, and test data covers 2013 onwards.\n",
    "# (This is roughly a 62.5%/12%/25% split.)\n",
    "train = mosquito[mosquito['Date'] < '2012-01-01']\n",
    "valid = mosquito[(mosquito['Date'] >= '2012-01-01') & (df['Date'] < '2013-01-01')]\n",
    "test  = mosquito[mosquito['Date'] >= '2013-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f7d41487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['zip_code'] = train.Address.apply(find_zip)\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid['zip_code'] = valid.Address.apply(find_zip)\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9368\\1888388000.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['zip_code'] = test.Address.apply(find_zip)\n"
     ]
    }
   ],
   "source": [
    "# Extract zip codes; this will be the main location based feature outside of lat/long\n",
    "train['zip_code'] = train.Address.apply(find_zip)\n",
    "valid['zip_code'] = valid.Address.apply(find_zip)\n",
    "test['zip_code'] = test.Address.apply(find_zip)\n",
    "\n",
    "# Address, Street, and AddressNumberAndStreet provide the same information, so they are removed to \n",
    "# avoid redundancy.\n",
    "train = train.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)\n",
    "valid = valid.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)\n",
    "test = test.drop(['Address', 'Street', 'AddressNumberAndStreet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "20f75c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing zip codes based on lat/long. A unique strategy is used here, where I train a \n",
    "# model to do this based on the training latitude and longitude data.\n",
    "X = train.dropna(subset=['Latitude', 'Longitude', 'zip_code'])[['Latitude', 'Longitude']]\n",
    "y = train.dropna(subset=['Latitude', 'Longitude', 'zip_code'])['zip_code']\n",
    "\n",
    "# Fit model to predict zip codes based on lat/long\n",
    "dtree_forzip = tree.DecisionTreeClassifier()\n",
    "dtree_forzip.fit(X, y)\n",
    "\n",
    "# Define a mask to look for rows with missing zip codes, and predict them for train set\n",
    "mask_zip_null = train['zip_code'].isnull()\n",
    "X_missing_zip = train.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "train.loc[mask_zip_null, 'zip_code'] = predicted_zip\n",
    "\n",
    "# Repeat for valid and test sets\n",
    "mask_zip_null = valid['zip_code'].isnull()\n",
    "X_missing_zip = valid.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "valid.loc[mask_zip_null, 'zip_code'] = predicted_zip\n",
    "\n",
    "mask_zip_null = test['zip_code'].isnull()\n",
    "X_missing_zip = test.loc[mask_zip_null, ['Latitude', 'Longitude']]\n",
    "predicted_zip = dtree_forzip.predict(X_missing_zip)\n",
    "test.loc[mask_zip_null, 'zip_code'] = predicted_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ee4ecd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of year, month, day of week, and year from date column\n",
    "train['day_of_year'] = train.Date.dt.dayofyear\n",
    "train['month'] = train.Date.dt.month\n",
    "train['day_of week'] = train.Date.dt.dayofweek\n",
    "train['year'] = train.Date.dt.year\n",
    "\n",
    "valid['day_of_year'] = valid.Date.dt.dayofyear\n",
    "valid['month'] = valid.Date.dt.month    \n",
    "valid['day_of week'] = valid.Date.dt.dayofweek\n",
    "valid['year'] = valid.Date.dt.year\n",
    "\n",
    "test['day_of_year'] = test.Date.dt.dayofyear\n",
    "test['month'] = test.Date.dt.month\n",
    "test['day_of week'] = test.Date.dt.dayofweek\n",
    "test['year'] = test.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e7efc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I create some new features based on trap data. The idea is to create features that capture \n",
    "# the distribution of mosquitos and WNV presence across different traps. This can help the model \n",
    "# understand the relative importance of different traps in terms of mosquito counts and WNV \n",
    "# presence.\n",
    "num_by_trap = train[['Trap', 'NumMosquitos', 'WnvPresent']].groupby('Trap').agg('sum')\n",
    "num_by_trap['trap_percent_of_all_mosquitos'] = num_by_trap['NumMosquitos']/sum(num_by_trap.NumMosquitos)\n",
    "num_by_trap['trap_percent_with_wnv'] = num_by_trap.WnvPresent/num_by_trap.NumMosquitos\n",
    "num_by_trap.reset_index(inplace=True)\n",
    "\n",
    "# Define dictionaries to map trap names to their respective weights\n",
    "map_mosq_weight = {t:v for t, v in zip(num_by_trap.Trap.values, num_by_trap['trap_percent_of_all_mosquitos'].values)}\n",
    "map_wnv_weight = {t:v for t, v in zip(num_by_trap.Trap.values, num_by_trap['trap_percent_with_wnv'].values)}\n",
    "\n",
    "# Create a second mapping for WNV presence, which is the average per-record WNV presence per trap\n",
    "map_wnv_weight2 = {}\n",
    "for trap in set(train.Trap):\n",
    "    map_wnv_weight2[trap] = sum(train.WnvPresent[train.Trap==trap]/sum(train.Trap==trap))\n",
    "map_wnv_weight2\n",
    "\n",
    "# Map the trap weights to the train, valid, and test sets\n",
    "train['trap_percent_of_all_mosquitos'] = train.Trap.map(map_mosq_weight)\n",
    "train['trap_percent_with_wnv'] = train.Trap.map(map_wnv_weight)\n",
    "train['trap_percent_with_wnv2'] = train.Trap.map(map_wnv_weight2)\n",
    "\n",
    "valid['trap_percent_of_all_mosquitos'] = valid.Trap.map(map_mosq_weight).fillna(0)\n",
    "valid['trap_percent_with_wnv'] = valid.Trap.map(map_wnv_weight).fillna(0)\n",
    "valid['trap_percent_with_wnv2'] = valid.Trap.map(map_wnv_weight2).fillna(0)\n",
    "\n",
    "test['trap_percent_of_all_mosquitos'] = test.Trap.map(map_mosq_weight).fillna(0)\n",
    "test['trap_percent_with_wnv'] = test.Trap.map(map_wnv_weight).fillna(0)\n",
    "test['trap_percent_with_wnv2'] = test.Trap.map(map_wnv_weight2).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "84fecd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the mosquito data with the combined weather data, left joining on the date column\n",
    "train_w = pd.merge(train, combined_weather, how='left', on='Date')\n",
    "valid_w = pd.merge(valid, combined_weather, how='left', on='Date')\n",
    "test_w = pd.merge(test, combined_weather, how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6ebd6db6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 15)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[194]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m mask_wb_null = valid_w[\u001b[33m'\u001b[39m\u001b[33mWetBulb\u001b[39m\u001b[33m'\u001b[39m].isnull()\n\u001b[32m     17\u001b[39m X_pred = valid_w.loc[mask_wb_null, X.columns]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m predicted_WetBulb = \u001b[43mdt_WetBulb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m valid_w.loc[mask_wb_null, \u001b[33m'\u001b[39m\u001b[33mWetBulb\u001b[39m\u001b[33m'\u001b[39m] = predicted_WetBulb\n\u001b[32m     21\u001b[39m mask_wb_null = test_w[\u001b[33m'\u001b[39m\u001b[33mWetBulb\u001b[39m\u001b[33m'\u001b[39m].isnull()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kevin\\OneDrive\\Documents\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:1066\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1064\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1069\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kevin\\OneDrive\\Documents\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:638\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kevin\\OneDrive\\Documents\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kevin\\OneDrive\\Documents\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 15)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "# Impute missing values for WetBulb and StnPressure using a model-based approach, similar to above\n",
    "\n",
    "# WetBulb\n",
    "X = combined_weather.dropna().drop(['Date', 'WetBulb', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['WetBulb']\n",
    "\n",
    "dt_WetBulb = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "dt_WetBulb.fit(X, y)\n",
    "\n",
    "# Predict missing WetBulb values\n",
    "mask_wb_null = train_w['WetBulb'].isnull()\n",
    "X_pred = train_w.loc[mask_wb_null, X.columns]\n",
    "predicted_WetBulb = dt_WetBulb.predict(X_pred)\n",
    "train_w.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\n",
    "\n",
    "mask_wb_null = valid_w['WetBulb'].isnull()\n",
    "X_pred = valid_w.loc[mask_wb_null, X.columns]\n",
    "predicted_WetBulb = dt_WetBulb.predict(X_pred)\n",
    "valid_w.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\n",
    "\n",
    "mask_wb_null = test_w['WetBulb'].isnull()\n",
    "X_pred = test_w.loc[mask_wb_null, X.columns]\n",
    "predicted_WetBulb = dt_WetBulb.predict(X_pred)\n",
    "test_w.loc[mask_wb_null, 'WetBulb'] = predicted_WetBulb\n",
    "\n",
    "# StnPressure\n",
    "X = combined_weather.dropna().drop(['Date', 'StnPressure', 'CodeSum'], axis=1)\n",
    "y = combined_weather.dropna()['StnPressure']\n",
    "\n",
    "xb_StnPressure = xgboost.XGBRegressor()\n",
    "xb_StnPressure.fit(X, y)\n",
    "\n",
    "# Predict missing StnPressure values\n",
    "mask_sp_null = train_w['StnPressure'].isnull()\n",
    "X_pred = train_w.loc[mask_sp_null, X.columns]\n",
    "predicted_StnPressure = xb_StnPressure.predict(X_pred)\n",
    "train_w.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure\n",
    "\n",
    "mask_sp_null = valid_w['StnPressure'].isnull()\n",
    "X_pred = valid_w.loc[mask_sp_null, X.columns]\n",
    "predicted_StnPressure = xb_StnPressure.predict(X_pred)\n",
    "valid_w.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure\n",
    "\n",
    "mask_sp_null = test_w['StnPressure'].isnull()\n",
    "X_pred = test_w.loc[mask_sp_null, X.columns]\n",
    "predicted_StnPressure = xb_StnPressure.predict(X_pred)\n",
    "test_w.loc[mask_sp_null, 'StnPressure'] = predicted_StnPressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "286d7b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_w:\n",
      "NumMosquitos              67055\n",
      "WnvPresent                67055\n",
      "Id                         8114\n",
      "trap_percent_with_wnv      6618\n",
      "trap_percent_with_wnv2    75169\n",
      "StnPressure                  93\n",
      "dtype: int64\n",
      "\n",
      "Missing values in valid_w:\n",
      "NumMosquitos    27115\n",
      "WnvPresent      27115\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test_w:\n",
      "NumMosquitos    22123\n",
      "WnvPresent      22123\n",
      "Id               2392\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in train_w:\")\n",
    "print(train_w.isnull().sum()[train_w.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nMissing values in valid_w:\")\n",
    "print(valid_w.isnull().sum()[valid_w.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nMissing values in test_w:\")\n",
    "print(test_w.isnull().sum()[test_w.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0670d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(50)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w.NumMosquitos.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
